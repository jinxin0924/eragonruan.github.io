---
layout:     post
title:      "AutoML总结"
date:       2017-12-21 14:48:00
author:     "JxKing"
header-img: "img/post-bg-001.jpg"
catalog: true
tags:
    - AutoML
---

## 前言

AutoML是指尽量不通过人来设定超参数，而是使用某种学习机制，来调节这些超参数。这些学习机制包括传统的贝叶斯优化，多臂老虎机（multi-armed bandit），进化算法，还有比较新的强化学习。

我将AutoML分为**传统AutoML** ，自动调节传统的机器学习算法的参数，比如随机森林，我们来调节它的max_depth, num_trees, criterion等参数。 还有一类AutoML，则专注深度学习。这类AutoML，不妨称之为**深度AutoML** ，与传统AutoML的差别是，现阶段深度AutoML，会将神经网络的超参数分为两类，一类是与训练有关的超参数，比如learning rate, regularization, momentum等；还有一类超参数，则可以总结为网络结构。对网络结构的超参数自动调节，也叫 **Neural architecture search (nas)** 。而针对训练的超参数，也是传统AutoML的自动调节，叫 **Hyperparameter optimization (ho)** 。



### 贝叶斯优化

贝叶斯优化是一种近似逼近的方法，用各种代理函数来拟合超参数与模型评价之间的关系，然后选择有希望的超参数组合进行迭代，最后得出效果最好的超参数组合。

#### 算法流程

1. 初始化，随机选择若干组参数x，训练模型，得到相应的模型评价指标y
2. 用代理函数来拟合x,y
3. 用采集函数来选择最佳的x*
4. 将x*带入模型，得到 新的y，然后进入第2步

#### 具体算法

| 算法                                       | 代理函数   | 采集函数                   | 优缺点               |
| ---------------------------------------- | ------ | ---------------------- | ----------------- |
| [**BO**](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) | 高斯过程   | Expected Improvement   | 应用广泛，在低维空间表现出色    |
| [**SMAC**](http://www.cs.ubc.ca/~hutter/papers/11-LION5-SMAC.pdf) | 回归随机森林 | Upper Confidence Bound | 对离散型变量表现出色        |
| [**TPE**](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) | 高斯混合模型 | Expected Improvement   | 高维空间表现出色，有论文表明最实用 |

#### 特点

* 需要消耗大量资源及时间。由于需要至少几十次迭代，即需要训练几十次的模型，因而会造成大量资源、时间消耗。基于这个特点，可以说贝叶斯优化算法**适合传统AutoML** ，而**不适合深度AutoML**

* 效果不稳定。由于初始化存在随机性，其效果不稳定。也有论文表明，贝叶斯优化算法并不显著优于随机搜索(random search)

  ​



## Multi-armed Bandit

multi-armed bandit是非常经典的序列决策模型，要解决的问题是平衡“探索”(exploration)和“利用”(exploitation)。

*举一个bandit例子，你有20个按钮，每个按钮按一次可能得到一块钱或者拿不到钱，同时每个按钮的得到一块钱的概率不同，而你在事前对这些概率一无所知。在你有1000次按按钮的机会下，呼和得到最大收益。* 

这类算法，通过将自动调参问题，转化为bandit问题，配置更多资源给表现更优异的参数模型。

#### 具体算法

[Hyperband](https://arxiv.org/pdf/1603.06560.pdf)是一个颇具代表的算法。总体思路我们由一个自动调节LeNet的例子来展示：

![hyperband](https://jinxin0924.github.io/img/in-post/post1/hyperband.PNG)

R=81代表总资源，$\mu$ 代表每次筛选的比例，ni代表参数配置的组合数，ri代表资源数，这里代表一个epoch，第一行代表随机得到ni个参数配置，然后经过第ri次迭代之后，根据模型validation loss选择出top k个表现好的模型，继续下一行ri的训练。

#### 特点

1. Bandit思想还是很重要的，是一类针对资源配置的算法，可以有效避免资源浪费在很差的参数配置上。
2. Bandit结合贝叶斯优化，就构成了传统的AutoML的核心，比如伯克利的这篇[paper](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/07/163-sparks.pdf)，或者今年cmu的[atm](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/07/163-sparks.pdf) 。
3. Bandit同样适合于深度AutoML中nas任务，但是对ho任务的应用，我是存疑的，比如学习率lr 一大一小两组实验，在前期极有可能是大lr那组的loss下降快，如果用bandit判断这个lr优秀，而停止了另一组的实验，很有可能造成错误。因为大的学习率，在前期可能确实会加快收敛，但是一段时间后，可能就会震荡了，最后的收敛精度可能就很低。




## 进化算法

一般的进化算法其实大同小异，差别在如何选择变异，有比较细的变异，比如在[Large-Scale Evolution of Image Classifiers](https://arxiv.org/pdf/1703.01041.pdf) 这篇文章中，就定义了非常具体的变异，比如有改变通道数量，改变filter大小，改变stride等等；而在[SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/pdf/1711.04528.pdf)这篇论文中，它的变异，就借鉴了现有公认的比较好的结构，加深网络就用conv-bn-relu3件套，加宽网络加大通道数量，增加skip connection。

这些进化算法在做自动模型选择时，每次迭代都不可避免的需要在整个数据集上跑若干个epoch，而每次迭代都有许多个变异，又需要很多次迭代，导致最后的训练时间太久。

### fine-tune基础上的进化

[SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/pdf/1711.04528.pdf)这篇论文提出，我们先用一个成熟的模型去训练(也可以fine-tune训练)，然后在这个模型的基础上去变异，变异之后用fine-tune训练几个epoch即可。这带来两个好的结果：

1. fine tune减少了大量的训练时间
2. 我们最后拿出来的模型，至少不比成熟模型差

个人认为，这篇论文很有实际意义。






