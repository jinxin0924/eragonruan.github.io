---
layout:     post
title:      "RNN总结"
date:       2017-12-28 14:48:00
author:     "JxKing"
header-img: "img/post-bg-001.jpg"
catalog: true
tags:
    - RNN
---

## RNN的简单介绍

[参考](cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

## 简单RNN的计算

### 前向计算
[参考](blog.csdn.net/diligent_321/article/details/53365621)


### 后向计算



## LSTM



## Seq2Seq



## Attention

从nmt角度来看，Attention是为了解决传统seq2seq长输入序列问题。

传统seq2seq会将输入序列不论长短，通过rnn编码成一个固定长度的状态向量(state vector)，这就导致，如果输入序列过长，那么这个向量难以保留全部的必要信息，导致后续解码时输入信息不足，模型性能差。

Attention机制其实很简单，既然只使用最后一个状态向量会造成信息损失，那么干脆把之前的状态向量都用好了，而attention机制起的作用就是**选择恰当的状态向量**。在大部分论文中，Attentions的实现是通过**训练一个模型**，输入是所有的encoder阶段的状态向量，输出是一个权重向量，代表各个输入的权重，权重越大，代表该输入越重要。





```
def lossFun(inputs, targets, hprev):
  """
  inputs,targets are both list of integers.
  hprev is Hx1 array of initial hidden state
  returns the loss, gradients on model parameters, and last hidden state
  """
  xs, hs, ys, ps = {}, {}, {}, {}
  hs[-1] = np.copy(hprev)
  loss = 0
  # forward pass
  for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
  # backward pass: compute gradients going backwards
  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
  dbh, dby = np.zeros_like(bh), np.zeros_like(by)
  dhnext = np.zeros_like(hs[0])
  for t in reversed(xrange(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients
  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]
```
